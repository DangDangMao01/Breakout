"""
å¤šæ¨¡å‹ CLI - æ”¯æŒåˆ‡æ¢ä¸åŒçš„ AI æ¨¡å‹
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

import argparse
from core.llm_client import OllamaClient
from agents.web_enhanced_agent import WebEnhancedAgent
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MultiModelCLI:
    """å¤šæ¨¡å‹ CLI"""
    
    def __init__(self, host="http://localhost:11434"):
        self.host = host
        self.current_model = None
        self.llm = None
        self.agent = None
        self.available_models = []
        
    def load_available_models(self):
        """åŠ è½½å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            import requests
            response = requests.get(f"{self.host}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [m["name"] for m in data.get("models", [])]
                return True
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return False
    
    def switch_model(self, model_name: str):
        """åˆ‡æ¢æ¨¡å‹"""
        if model_name not in self.available_models:
            print(f"âŒ æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨")
            print(f"å¯ç”¨æ¨¡å‹: {', '.join(self.available_models)}")
            return False
        
        print(f"\nğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
        self.current_model = model_name
        
        # é‡æ–°åˆ›å»º LLM å®¢æˆ·ç«¯
        self.llm = OllamaClient(host=self.host, model=model_name)
        
        # é‡æ–°åˆ›å»ºæ™ºèƒ½ä½“
        try:
            self.agent = WebEnhancedAgent(self.llm)
            print(f"âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"åˆ›å»ºæ™ºèƒ½ä½“å¤±è´¥: {e}")
            return False
    
    def show_models(self):
        """æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        print("=" * 60)
        
        for i, model in enumerate(self.available_models, 1):
            current = "âœ“" if model == self.current_model else " "
            print(f"[{current}] {i}. {model}")
        
        print("=" * 60)
    
    def compare_models(self, question: str, models: list = None):
        """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„å›ç­”"""
        if not models:
            models = self.available_models[:3]  # é»˜è®¤å¯¹æ¯”å‰ 3 ä¸ª
        
        print("\n" + "=" * 60)
        print(f"é—®é¢˜: {question}")
        print("=" * 60)
        
        results = {}
        for model in models:
            print(f"\nğŸ¤– {model} çš„å›ç­”:")
            print("-" * 60)
            
            # ä¸´æ—¶åˆ‡æ¢æ¨¡å‹
            temp_llm = OllamaClient(host=self.host, model=model)
            try:
                answer = temp_llm.chat(question)
                results[model] = answer
                print(answer)
            except Exception as e:
                results[model] = f"é”™è¯¯: {e}"
                print(f"âŒ {e}")
            
            print("-" * 60)
        
        return results
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print(f"\n{'=' * 60}")
        print(f"å¤šæ¨¡å‹ AI åŠ©æ‰‹")
        print(f"{'=' * 60}")
        print("åŠŸèƒ½ï¼š")
        print("  - æ”¯æŒå¤šä¸ª AI æ¨¡å‹")
        print("  - éšæ—¶åˆ‡æ¢æ¨¡å‹")
        print("  - å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å›ç­”")
        print("  - ç½‘ç»œæœç´¢å¢å¼º")
        print(f"{'=' * 60}")
        print("\nå‘½ä»¤ï¼š")
        print("  quit          - é€€å‡ºç¨‹åº")
        print("  models        - æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹")
        print("  switch <æ¨¡å‹> - åˆ‡æ¢æ¨¡å‹")
        print("  compare <é—®é¢˜> - å¯¹æ¯”å¤šä¸ªæ¨¡å‹")
        print("  search        - åˆ‡æ¢æœç´¢æ¨¡å¼")
        print("  clear         - æ¸…ç©ºå†å²")
        print("  help          - æ˜¾ç¤ºå¸®åŠ©")
        print(f"{'=' * 60}\n")
        
        force_search = False
        
        while True:
            try:
                # æ˜¾ç¤ºå½“å‰æ¨¡å‹
                prompt = f"[{self.current_model}]> " if self.current_model else "AI> "
                user_input = input(prompt).strip()
                
                if not user_input:
                    continue
                
                # å‘½ä»¤å¤„ç†
                if user_input.lower() == 'quit':
                    print("\nå†è§ï¼ğŸ‘‹")
                    break
                
                if user_input.lower() == 'models':
                    self.show_models()
                    continue
                
                if user_input.lower().startswith('switch '):
                    model_name = user_input[7:].strip()
                    self.switch_model(model_name)
                    continue
                
                if user_input.lower().startswith('compare '):
                    question = user_input[8:].strip()
                    self.compare_models(question)
                    continue
                
                if user_input.lower() == 'clear':
                    if self.llm:
                        self.llm.clear_history()
                        print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
                    continue
                
                if user_input.lower() == 'search':
                    force_search = not force_search
                    status = "å¼€å¯" if force_search else "å…³é—­"
                    print(f"âœ… å¼ºåˆ¶æœç´¢æ¨¡å¼å·²{status}")
                    continue
                
                if user_input.lower() == 'help':
                    print("\nå¯ç”¨å‘½ä»¤ï¼š")
                    print("  quit          - é€€å‡ºç¨‹åº")
                    print("  models        - æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹")
                    print("  switch <æ¨¡å‹> - åˆ‡æ¢æ¨¡å‹ï¼ˆå¦‚: switch qwen2.5:7bï¼‰")
                    print("  compare <é—®é¢˜> - å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„å›ç­”")
                    print("  search        - åˆ‡æ¢å¼ºåˆ¶æœç´¢æ¨¡å¼")
                    print("  clear         - æ¸…ç©ºå¯¹è¯å†å²")
                    print("  help          - æ˜¾ç¤ºå¸®åŠ©")
                    print()
                    continue
                
                # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†æ¨¡å‹
                if not self.current_model:
                    print("âš ï¸ è¯·å…ˆé€‰æ‹©æ¨¡å‹")
                    print("ä½¿ç”¨ 'models' æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
                    print("ä½¿ç”¨ 'switch <æ¨¡å‹å>' åˆ‡æ¢æ¨¡å‹")
                    continue
                
                # å›ç­”é—®é¢˜
                print()
                if force_search:
                    result = self.agent._search_web(user_input)
                    if result.get("success"):
                        print(result["summary"])
                    else:
                        print(f"æœç´¢å¤±è´¥ï¼Œä½¿ç”¨æ¨¡å‹çŸ¥è¯†å›ç­”ï¼š\n{self.agent.chat(user_input)}")
                else:
                    answer = self.agent.answer_with_search(user_input)
                    print(answer)
                print()
                
            except KeyboardInterrupt:
                print("\n\nå†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                logger.error(f"é”™è¯¯: {e}")
                print(f"\nâŒ é”™è¯¯: {e}\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šæ¨¡å‹ AI åŠ©æ‰‹ CLI")
    parser.add_argument(
        "--host",
        default="http://localhost:11434",
        help="Ollama æœåŠ¡åœ°å€"
    )
    parser.add_argument(
        "--model",
        help="é»˜è®¤æ¨¡å‹"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("åˆå§‹åŒ–å¤šæ¨¡å‹ AI åŠ©æ‰‹...")
    print("=" * 60)
    
    # åˆ›å»º CLI
    cli = MultiModelCLI(host=args.host)
    
    # åŠ è½½å¯ç”¨æ¨¡å‹
    print("\næ£€æŸ¥ Ollama è¿æ¥...")
    if not cli.load_available_models():
        print("âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
        print("è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(cli.available_models)} ä¸ªæ¨¡å‹")
    
    # é€‰æ‹©é»˜è®¤æ¨¡å‹
    if args.model:
        cli.switch_model(args.model)
    elif cli.available_models:
        # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹
        cli.switch_model(cli.available_models[0])
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        print("è¯·å…ˆä¸‹è½½æ¨¡å‹: ollama pull qwen2.5:7b")
        return
    
    # å¯åŠ¨äº¤äº’æ¨¡å¼
    cli.interactive_mode()


if __name__ == "__main__":
    main()
